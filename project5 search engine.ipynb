{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 5: Search Engine\n",
    "The first step in creating a search engine is to develop a way to collect the documents. After you collect the documents, they need to be indexed. The final step is, of course, returning a ranked list of documents from a query. Finally, you’ll build a neural network for ranking queries. \n",
    "\n",
    "中文范例   http://www.jianshu.com/p/42e12d502cc6       http://www.jianshu.com/p/e9f79d69a375"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### searchengine module: \n",
    "it has two classes: one for crawling and creating the database, and the other for doing full-text searches by querying the database.\n",
    "\n",
    "Crawling or spidering: it will be seeded with a small set of pages to index and will then follow any links on that page to find other pages, whose links it will also follow. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\r\\n<!DOCTYPE html\\r\\n  PUBLIC \"-//W3C//DTD HTML 4.01//EN\" \"http://www.w3.org/TR/html4/strict.dtd\">\\r\\n<html lang=\"en\">\\r\\n   <head>\\r\\n      <meta http-equiv=\"Content-Type\" content=\"text/html; charset=ISO-8859'\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "c = urllib.request.urlopen('http://www.diveintopython.net')\n",
    "contents = c.read()\n",
    "print(contents[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is', 'it', 'to', 'of', 'in', 'and', 'the', 'a'}\n"
     ]
    }
   ],
   "source": [
    "ignorewords = set(['the', 'of', 'to', 'and', 'of', 'a', 'in','is', 'it'])\n",
    "print(ignorewords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def crawl(self, pages, depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a name=\"intro\"></a>, <a class=\"plain\" href=\"http://www.amazon.com/gp/product/B001GIOFN8/ref=as_li_tf_tl?ie=UTF8&amp;tag=diveintopython-20&amp;linkCode=as2&amp;camp=1789&amp;creative=9325&amp;creativeASIN=B001GIOFN8\"><img alt=\"[Dive Into Python]\" height=\"140\" src=\"images/cover-small.jpg\" style=\"border: 0\" title=\"Dive Into Python: Buy it at Amazon.com\" width=\"106\"/></a>, <a href=\"toc/index.html\">read the book</a>, <a href=\"#download\" title=\"Download Dive Into Python\">download it</a>, <a title=\"Dive Into Python in your language\">multiple languages</a>, <a name=\"read\"></a>, <a href=\"toc/index.html\"><i class=\"citetitle\">Dive Into <span class=\"application\">Python</span></i></a>, <a href=\"appendix/history.html\">read the revision history</a>, <a href=\"mailto:josh@djazzee.com\">Email me</a>, <a name=\"download\"></a>]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(contents, 'lxml')\n",
    "links = soup('a')\n",
    "print(links[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'intro'}\n",
      "{'name': 'intro'}\n",
      "True\n",
      "{'class': ['plain'], 'href': 'http://www.amazon.com/gp/product/B001GIOFN8/ref=as_li_tf_tl?ie=UTF8&tag=diveintopython-20&linkCode=as2&camp=1789&creative=9325&creativeASIN=B001GIOFN8'}\n",
      "{'class': ['plain'], 'href': 'http://www.amazon.com/gp/product/B001GIOFN8/ref=as_li_tf_tl?ie=UTF8&tag=diveintopython-20&linkCode=as2&camp=1789&creative=9325&creativeASIN=B001GIOFN8'}\n",
      "True\n",
      "{'href': 'toc/index.html'}\n",
      "{'href': 'toc/index.html'}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for link in links[:3]:\n",
    "    print(link.attrs)\n",
    "    print(dict(link.attrs))\n",
    "    print(link.attrs == dict(link.attrs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### urllib.parse\n",
    "https://docs.python.org/3/library/urllib.parse.html\n",
    "urllib.parse.urljoin(base, url, allow_fragments=True)\n",
    "\n",
    "Construct a full (“absolute”) URL by combining a “base URL” (base) with another URL (url). Informally, this uses components of the base URL, in particular the addressing scheme, the network location and (part of) the path, to provide missing components in the relative URL.\n",
    "\n",
    "The allow_fragments argument has the same meaning and default as for urlparse().\n",
    "Note If url is an absolute URL (that is, starting with // or scheme://), the url’s host name and/or scheme will be present in the result. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.amazon.com/gp/product/B001GIOFN8/ref=as_li_tf_tl?ie=UTF8&tag=diveintopython-20&linkCode=as2&camp=1789&creative=9325&creativeASIN=B001GIOFN8\n",
      "['http://www.amazon.com/gp/product/B001GIOFN8/ref=as_li_tf_tl?ie=UTF8&tag=diveintopython-20&linkCode=as2&camp=1789&creative=9325&creativeASIN=B001GIOFN8']\n",
      "http://www.diveintopython.net/toc/index.html\n",
      "['http://www.diveintopython.net/toc/index.html']\n",
      "http://www.diveintopython.net#download\n",
      "['http://www.diveintopython.net', 'download']\n",
      "***  newpages set: ***\n",
      "{'http://www.diveintopython.net', 'http://www.diveintopython.net/toc/index.html', 'http://www.amazon.com/gp/product/B001GIOFN8/ref=as_li_tf_tl?ie=UTF8&tag=diveintopython-20&linkCode=as2&camp=1789&creative=9325&creativeASIN=B001GIOFN8'}\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse  \n",
    "page = 'http://www.diveintopython.net'\n",
    "newpages = set()\n",
    "for link in links[:5]:\n",
    "    if('href' in link.attrs):\n",
    "        url = urllib.parse.urljoin(page, link['href'])\n",
    "        print(url)\n",
    "        if url.find(\"'\") != -1:  # 如果包含子字符串返回开始的索引值，否则返回-1\n",
    "           continue\n",
    "        print(url.split('#'))\n",
    "        url = url.split('#')[0]  # remove location portion\n",
    "        #if url[:4] == 'http' and not self.isindexed(url):\n",
    "        if url[:4] == 'http':\n",
    "            newpages.add(url)\n",
    "print('***  newpages set: ***')\n",
    "print(newpages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### separate words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'test', '123', 'and', '456', 'Yes', 'or', 'no', 'yes', '']\n",
      "['This', 'is', 'a', 'test', '123', 'and', '456', 'Yes', 'or', 'no', 'yes', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'test', '123', 'and', '456', 'yes', 'or', 'no', 'yes']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "#def separatewords(self,text):\n",
    "text = 'This is a test. 123 and 456? Yes, or no. yes!'\n",
    "#\\W: non-word character,非数字和字母的字符\n",
    "#使用 compile 函数将正则表达式的字符串形式编译为一个 Pattern 对象\n",
    "splitter=re.compile('\\\\W+')  # 若是\\W* 0次或多次，会有futurewarning\n",
    "print(splitter.split(text))  # 最后都会有一个空字符‘’\n",
    "print(re.split('\\W+', text)) \n",
    "[s.lower() for s in re.split('\\W+', text) if s!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17,)\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "# getentryid\n",
    "con = sqlite3.connect('searchindex.db')\n",
    "cur = con.execute(\"select rowid from wordlist where word = 'python' \") # sql语句必须为双引号“”\n",
    "res = cur.fetchone()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
